import torch
import torch.nn.functional as F
import numpy as np
import cv2
import os
import uuid
import base64

from PIL import Image
from torchvision import transforms
from ultralytics import YOLO
from pathlib import Path
from typing import Union, List, Tuple, Optional, Dict, Any

class VectorService:
    def __init__(self, model_path: Optional[str] = None):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Standard image transformation - same as in gun_classification.py
        self.transform = transforms.Compose([
            transforms.Resize((640, 640)),
            transforms.ToTensor(),
        ])
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á paths ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πà‡∏≤‡∏á‡πÜ
        backend_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        
        # üìÅ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Segmentation
        self.segment_model_path = os.path.join(backend_dir, "app", "model", "Model V2", "5M_Segment", "best_v8.pt")
        self.segment_model = None
        
        # üìÅ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Feature Extraction
        self.narcotic_model_path = os.path.join(backend_dir, "app", "model", "Model V2", "Narcotics", "best.pt")
        if model_path:
            self.narcotic_model_path = model_path
            
        self.narcotic_model = None
        
        # üìÅ ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà segment ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ debug
        self.debug_dir = os.path.join(backend_dir, "debug_images")
        if not os.path.exists(self.debug_dir):
            os.makedirs(self.debug_dir)
            
        # üìä ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö class mapping
        self.segment_classes = {0: 'BigGun', 1: 'Bullet', 2: 'Drug', 3: 'Magazine', 4: 'PackageDrug', 5: 'Pistol', 6: 'Revolver'}
        
        # ‚öôÔ∏è ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
        self.load_segmentation_model()
        self.load_narcotic_model()
    
    def load_segmentation_model(self) -> None:
        """‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö segmentation"""
        try:
            if os.path.exists(self.segment_model_path):
                self.segment_model = YOLO(self.segment_model_path)
            else:
                pass
        except Exception as e:
            pass
    
    def load_narcotic_model(self) -> None:
        """‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á feature vector"""
        try:
            if os.path.exists(self.narcotic_model_path):
                self.narcotic_model = YOLO(self.narcotic_model_path)
            else:
                pass
        except Exception as e:
            pass
    
    def crop_mask_on_white(self, img: np.ndarray, mask: np.ndarray) -> np.ndarray:
        """
        ‡∏ï‡∏±‡∏î‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ï‡∏≤‡∏° mask ‡πÅ‡∏•‡∏∞‡∏ß‡∏≤‡∏á‡∏ö‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå gun_classification.py
        """
        # Resize mask ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö shape ‡∏Ç‡∏≠‡∏á image
        if mask.shape != img.shape[:2]:
            mask = cv2.resize(mask.astype(np.uint8), (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)

        white_bg = np.ones_like(img) * 255
        mask = mask.astype(bool)

        # Apply mask to each channel
        for c in range(3):
            white_bg[:, :, c][mask] = img[:, :, c][mask]
        return white_bg
    
    def process_image_for_vector(self, 
                               image: Union[str, Path, Image.Image, np.ndarray, bytes], 
                               save_debug_image: bool = True) -> Tuple[np.ndarray, Dict]:
        """
        ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÇ‡∏î‡∏¢ segment ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏≤‡πÄ‡∏™‡∏û‡∏ï‡∏¥‡∏î‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô
        ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∂‡∏á‡∏™‡πà‡∏á‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ‡∏¢‡∏±‡∏á feature extraction
        
        Args:
            image: ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ
            save_debug_image: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£ debug
            
        Returns:
            cropped_image: ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏≤‡πÄ‡∏™‡∏û‡∏ï‡∏¥‡∏î‡πÅ‡∏•‡πâ‡∏ß
            result_info: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡πÄ‡∏ä‡πà‡∏ô confidence, class)
        """
        if self.segment_model is None:
            raise ValueError("Segmentation model not loaded. Please load the model first.")
            
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        if isinstance(image, str) or isinstance(image, Path):
            cv_image = cv2.imread(str(image))
        elif isinstance(image, Image.Image):
            cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        elif isinstance(image, np.ndarray):
            cv_image = image
        elif isinstance(image, bytes):
            import io
            pil_image = Image.open(io.BytesIO(image))
            cv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
        else:
            raise TypeError("Image must be a file path, PIL Image, numpy array, or bytes")
        
        # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• segment ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏
        results = self.segment_model(cv_image)[0]
        
        # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        result_info = {
            "found_drug": False,
            "confidence": 0.0,
            "class_name": "",
            "debug_image_path": None
        }
        
        # ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÉ‡∏î‡πÜ
        if len(results.boxes) == 0:
            return cv_image, result_info
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏≤‡πÄ‡∏™‡∏û‡∏ï‡∏¥‡∏î (Drug ‡∏´‡∏£‡∏∑‡∏≠ PackageDrug)
        drug_indices = []
        for i, box in enumerate(results.boxes):
            cls_id = int(box.cls[0].item())
            cls_name = self.segment_classes.get(cls_id, "Unknown")
            confidence = float(box.conf[0].item())
            
            if cls_name in ['Drug', 'PackageDrug']:
                drug_indices.append((i, cls_name, confidence))
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏¢‡∏≤‡πÄ‡∏™‡∏û‡∏ï‡∏¥‡∏î ‡πÉ‡∏ä‡πâ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏î‡∏¥‡∏°
        if not drug_indices:
            return cv_image, result_info
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏¢‡∏≤‡πÄ‡∏™‡∏û‡∏ï‡∏¥‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        drug_indices.sort(key=lambda x: x[2], reverse=True)
        best_match = drug_indices[0]
        idx, cls_name, confidence = best_match
        
        # ‡∏ï‡∏±‡∏î‡∏†‡∏≤‡∏û‡∏ï‡∏≤‡∏° mask
        mask = results.masks.data[idx].cpu().numpy()
        cropped = self.crop_mask_on_white(cv_image, mask)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ debug
        if save_debug_image:
            debug_path = os.path.join(self.debug_dir, f"cropped_{cls_name}_{uuid.uuid4()}.jpg")
            cv2.imwrite(debug_path, cropped)
            result_info["debug_image_path"] = debug_path
        
        # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        result_info.update({
            "found_drug": True,
            "confidence": round(confidence, 2),
            "class_name": cls_name
        })
        
        return cropped, result_info
    
    def image_to_vector(self, 
                       image: Union[str, Path, Image.Image, np.ndarray, bytes], 
                       normalize: bool = True,
                       segment_first: bool = True,
                       save_debug_image: bool = True) -> torch.Tensor:
        """
        ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô vector ‡πÇ‡∏î‡∏¢‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£ segment ‡∏Å‡πà‡∏≠‡∏ô
        
        Args:
            image: ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ
            normalize: ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ normalize vector ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            segment_first: ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ segment ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            save_debug_image: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£ debug
            
        Returns:
            vector: vector representation ‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
        """
        if self.narcotic_model is None:
            raise ValueError("Narcotic model not loaded. Please load the model first.")
        
        processed_image = image
        result_info = {}
        
        # ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ segment ‡∏Å‡πà‡∏≠‡∏ô
        if segment_first:
            try:
                processed_image, result_info = self.process_image_for_vector(
                    image, save_debug_image=save_debug_image
                )
            except Exception as e:
                # ‡πÉ‡∏ä‡πâ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏î‡∏¥‡∏°‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
                processed_image = image
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö feature extraction
        if isinstance(processed_image, np.ndarray):
            processed_image = Image.fromarray(cv2.cvtColor(processed_image, cv2.COLOR_BGR2RGB))
        elif isinstance(processed_image, str) or isinstance(processed_image, Path):
            processed_image = Image.open(processed_image).convert("RGB")
        elif isinstance(processed_image, bytes):
            import io
            processed_image = Image.open(io.BytesIO(processed_image)).convert("RGB")
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô tensor
        tensor = self.transform(processed_image).unsqueeze(0)
        tensor = tensor.to(self.device)
        
        # ‡∏™‡∏Å‡∏±‡∏î feature vector
        with torch.no_grad():
            backbone = self.narcotic_model.model.model[:-1]
            features = backbone(tensor)
            vector = features.view(features.size(0), -1)[0]
        
        # Normalize vector if requested
        if normalize:
            vector = F.normalize(vector, p=2, dim=0)
            
        return vector
    
    def torch_vector_resize(self, vector: torch.Tensor, target_dim: int = 16000) -> torch.Tensor:
        """
        ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î vector ‡∏î‡πâ‡∏ß‡∏¢ PyTorch operations ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ NumPy
        
        Args:
            vector: PyTorch tensor ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î
            target_dim: ‡∏°‡∏¥‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            
        Returns:
            PyTorch tensor ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        """
        current_dim = vector.size(0)
        
        # ‡∏ñ‡πâ‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç
        if current_dim == target_dim:
            return vector
        
        # ‡∏ñ‡πâ‡∏≤ vector ‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ target_dim ‡∏ó‡∏≥ padding ‡∏î‡πâ‡∏ß‡∏¢ 0
        if current_dim < target_dim:
            padded = torch.zeros(target_dim, device=vector.device)
            padded[:current_dim] = vector
            return padded
        
        # ‡∏ñ‡πâ‡∏≤ vector ‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏Å‡∏ß‡πà‡∏≤ target_dim ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏î‡πâ‡∏ß‡∏¢ adaptive avg pooling
        # ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ mean pooling ‡∏î‡πâ‡∏ß‡∏¢ NumPy
        vector_reshaped = vector.view(1, 1, -1)  # [1, 1, dim]
        pooled = F.adaptive_avg_pool1d(vector_reshaped, target_dim)
        return pooled.view(-1)  # [target_dim]

    def vector_to_numpy(self, vector: torch.Tensor, target_dim: int = 16000) -> np.ndarray:
        """
        ‡πÅ‡∏õ‡∏•‡∏á PyTorch tensor ‡πÄ‡∏õ‡πá‡∏ô numpy array ‡πÇ‡∏î‡∏¢‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏î‡πâ‡∏ß‡∏¢ PyTorch ‡∏Å‡πà‡∏≠‡∏ô
        """
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ vector ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if vector.numel() == 0 or (vector.numel() == 1 and vector.item() == 0):
            np.random.seed(42)  # ‡πÉ‡∏ä‡πâ seed ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà
            return np.random.normal(0, 0.1, target_dim).astype(np.float32)
        
        # ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î vector ‡∏î‡πâ‡∏ß‡∏¢ PyTorch operations
        resized_vector = self.torch_vector_resize(vector, target_dim)
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô NumPy array ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡πÅ‡∏•‡πâ‡∏ß
        return resized_vector.cpu().numpy()
    
    @staticmethod
    def calculate_similarity(vector1: Union[torch.Tensor, np.ndarray], 
                           vector2: Union[torch.Tensor, np.ndarray]) -> float:
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô PyTorch tensor ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà
        if isinstance(vector1, np.ndarray):
            vector1 = torch.from_numpy(vector1).float()
        if isinstance(vector2, np.ndarray):
            vector2 = torch.from_numpy(vector2).float()
            
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì cosine similarity
        return F.cosine_similarity(vector1.unsqueeze(0), vector2.unsqueeze(0)).item()
    
    def find_similar_images(self, 
                          query_image: Union[str, Path, Image.Image, np.ndarray],
                          database: Dict[str, Dict[str, Any]],
                          top_k: int = 5) -> List[Dict[str, Any]]:
        query_vector = self.image_to_vector(query_image)
        
        results = []
        for image_id, image_data in database.items():
            if "vector" in image_data:
                # ‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô list ‡∏´‡∏£‡∏∑‡∏≠ numpy array ‡πÉ‡∏ô database
                db_vector = image_data["vector"]
                if isinstance(db_vector, list):
                    db_vector = torch.tensor(db_vector)
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á
                similarity = self.calculate_similarity(query_vector, db_vector)
                
                results.append({
                    "id": image_id,
                    "similarity": round(similarity, 4),
                    "metadata": {k: v for k, v in image_data.items() if k != "vector"}
                })
        
        results.sort(key=lambda x: x["similarity"], reverse=True)
        return results[:top_k]
    
    def process_image_batch(self, 
                          images: List[Union[str, Path, Image.Image, np.ndarray]],
                          normalize: bool = True) -> List[torch.Tensor]:
        """
        Process a batch of images to vectors
        
        Args:
            images: List of images
            normalize: Whether to normalize vectors
            
        Returns:
            List of feature vectors (PyTorch tensors)
        """
        vectors = []
        for img in images:
            try:
                vector = self.image_to_vector(img, normalize)
                vectors.append(vector)
            except Exception as e:
                vectors.append(None)
        return vectors

_vector_service = VectorService()

def create_vector_embedding(image_data: Union[str, Path, Image.Image, np.ndarray, bytes], segment_first: bool = True) -> Dict:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á vector embedding ‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
    """
    global _vector_service
    
    if _vector_service.segment_model is None:
        _vector_service.load_segmentation_model()
        
    if _vector_service.narcotic_model is None:
        _vector_service.load_narcotic_model()
    
    result_info = {}
    try:
        if segment_first:
            processed_image, result_info = _vector_service.process_image_for_vector(image_data)
            vector = _vector_service.image_to_vector(processed_image, normalize=True, segment_first=False)
        else:
            vector = _vector_service.image_to_vector(image_data, normalize=True, segment_first=False)
    except Exception as e:
        raise
    
    vector_np = _vector_service.vector_to_numpy(vector)
    
    # Convert vector to base64 instead of JSON list
    vector_bytes = vector_np.tobytes()
    vector_base64 = base64.b64encode(vector_bytes).decode('utf-8')

    result = {
        # Remove JSON list and use base64 instead
        "vector_base64": vector_base64,
        "vector_dimension": len(vector_np),
        "segmentation_result": result_info
    }
    
    return result

def preview_vector(vector: Union[torch.Tensor, np.ndarray]) -> None:
    """
    ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏≠‡∏á vector ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå gun_classification.py
    
    Args:
        vector: PyTorch tensor ‡∏´‡∏£‡∏∑‡∏≠ NumPy array ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    """
    # ‡πÅ‡∏õ‡∏•‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô PyTorch tensor ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô NumPy array
    if isinstance(vector, np.ndarray):
        vector = torch.from_numpy(vector).float()
    
    # 1. ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á vector
    print(f"Vector type: {type(vector)}")
    print(f"Vector shape: {vector.shape}")
    print(f"Vector size: {vector.numel()} elements")
    
    # 2. ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
    print(f"Min value: {vector.min().item()}")
    print(f"Max value: {vector.max().item()}")
    print(f"Mean value: {vector.mean().item()}")
    
    # 3. ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 10 ‡∏Ñ‡πà‡∏≤‡πÅ‡∏£‡∏Å
    print("\nFirst 10 values:")
    first_ten = vector[:10].cpu().numpy()
    for i, val in enumerate(first_ten):
        print(f"  [{i}] = {val}")
    
    # 4. ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 ‡∏Ñ‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    print("\nLast 5 values:")
    last_five = vector[-5:].cpu().numpy()
    for i, val in enumerate(last_five):
        idx = len(vector) - 5 + i
        print(f"  [{idx}] = {val}")
    
    # 5. ‡πÅ‡∏™‡∏î‡∏á‡∏Æ‡∏¥‡∏™‡πÇ‡∏ï‡πÅ‡∏Å‡∏£‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ï‡πà‡∏≤‡∏á‡πÜ (‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô 10 ‡∏ä‡πà‡∏ß‡∏á)
    hist_data = vector.cpu().numpy()
    hist, bin_edges = np.histogram(hist_data, bins=10)
    print("\nValue distribution (histogram):")
    for i in range(len(hist)):
        print(f"  [{bin_edges[i]:.3f} to {bin_edges[i+1]:.3f}]: {hist[i]} values")